---
layout: default
title: 'Machine Learning Diary 001: Swift detour, and Recreating FastAI''s Collaborative
Filtering'
date: '2020-10-02T14:04:00.001-07:00'
author: garymm
tags:
- machine learning
- programming
modified_time: '2020-10-02T14:04:16.693-07:00'
blogger_id: tag:blogger.com,1999:blog-1383463705740489547.post-418020651569958536
blogger_orig_url: https://blog.garymm.org/2020/10/machine-learning-diary-001-swift-detour.html
---

<p>I've been spending quite a bit of my free time studying machine learning over the past 3 months. After spending quite
  a bit of time deciding what learning materials to start with, I decided on fast.ai's Practical Deep Learning for
  Coders (I saw the&nbsp;<a href="https://course19.fast.ai/">2019 version</a>). Overall I highly recommend the course,
  and I'm planning to go back and do the 2020 version of it.</p>
<p>One reason I was excited about fast.ai's course is that they had some content on <a
    href="https://www.tensorflow.org/swift">Swift for TensorFlow</a>&nbsp;(S4TF). I have written a lot of Python and
  while it has its place, I was excited to try something new and hopefully better. I watched the two fast.ai videos on
  S4TF and then decided to try it out. This ended up being a fairly costly mistake for me. Basically S4TF is just not
  nearly as mature as PyTorch, and I wasted a bunch of time struggling with environment setup (no equivalent
  to&nbsp;<span style="font-family: courier;">conda install</span>), bugs, very incomplete documentation, and cryptic
  error messages. At this point (I tried in 2020-09), I suggest you steer clear of S4TF if your primary goal is just to
  learn ML. Though it was fun to learn some Swift and I'd definitely consider using Swift in future projects, ML or
  otherwise.</p>
<p>With that multi-week detour out of the way, I decided to focus on PyTorch. For my first task I decided to try to
  recreate the simplest model presented in the fast.ai course, but without using the fastai library at all, just pure
  PyTorch. I found that almost all of the special stuff in fastai is included in PyTorch as of 1.6, so this was a lot
  easier than I expected. Here's my final <a
    href="https://github.com/garymm/mlnbs/blob/master/collaborative-filtering.ipynb">notebook for this collaborative
    filtering task</a>. Some of the issues I ran into and how I resolved them:</p>
<p></p>
<ul style="text-align: left;">
  <li>For this task, using the GPU was much slower than just using the CPU. I suppose this is because the model is so
    simple that the overhead of copying data to/from the GPU dominates any speedup in actual calculations. In retrospect
    this was also probably why training my Swift version of the model was so slow, and <a
      href="https://github.com/tensorflow/swift/issues/524#issuecomment-680919423">S4TF doesn't allow eager tensors to
      use the CPU</a>&nbsp;if a GPU is present, so this would have been much harder to discover and work around in
    Swift.</li>
  <li>Fastai wraps the <span style="font-family: courier;">torch.nn.Embedding</span> type with a function that changes
    the initial values. PyTorch initializes using random values drawn from a normal distribution with&nbsp;ùúé^2 = 1.
    Fastai changes this to a normal distribution with&nbsp;ùúé^2 = .01, truncated so no value is farther than 2 standard
    deviations from the mean. They reference <a href="https://arxiv.org/abs/1711.09160">An Exploration of Word Embedding
      Initialization in Deep-Learning Tasks</a>&nbsp;as justification. I found this provided a small boost in
    performance.</li>
</ul>
<p></p>
<ul>
  <li>I forgot calls to <span style="font-family: courier;">squeeze()</span><span style="font-family: inherit;"> in a
      few places. I was quite surprised that </span><span style="font-family: courier;">torch.nn.MSELoss</span><span
      style="font-family: inherit;">&nbsp;didn't throw an error when I provided it two tensors of different shapes.
      Instead it did some averaging over all the values, which led to poor training performance.</span></li>
</ul>
